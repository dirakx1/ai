# Whats is the needed GPU MEM for running a LLM
* fitting models into the ram:  precision x mode parameters) .

# Quntization
* allows to run more powerful models in less RAM
* allows train larger model
* Reduce energy consumption
* bsically allows to train models with less precission but the output has the same performance.

# TGI 
* Text generation inference.
* allows to serve many LLM, for example llama 70b
